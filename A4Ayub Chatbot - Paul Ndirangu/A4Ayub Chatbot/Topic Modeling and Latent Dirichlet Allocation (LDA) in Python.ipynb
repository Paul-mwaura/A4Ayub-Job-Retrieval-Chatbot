{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Topic Modeling and Latent Dirichlet Allocation (LDA) in Python.ipynb","provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyN3a8JkNeQIHPX8mRQHMyRd"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"B2cVQb0XuolE","colab_type":"text"},"source":["# Topic Modeling and Latent Dirichlet Allocation (LDA) in Python"]},{"cell_type":"markdown","metadata":{"id":"E7k_egW5vBF4","colab_type":"text"},"source":["> **Topic modeling** is a type of statistical modeling for discovering the abstract “topics” that occur in a collection of documents. **Latent Dirichlet Allocation (LDA)** is an example of topic model and is used to classify text in a document to a particular topic. It builds a topic per document model and words per topic model, modeled as **Dirichlet distributions**."]},{"cell_type":"code","metadata":{"id":"LSoFvHdRC0Qz","colab_type":"code","colab":{}},"source":["# Import necessary Libraries.\n","#\n","import pandas as pd\n","\n","data = pd.read_csv('abcnews-date-text.csv', error_bad_lines=False);\n","data_text = data[['headline_text']]\n","data_text['index'] = data_text.index\n","documents = data_text"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZfBp_IcTUVGE","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":221},"executionInfo":{"status":"error","timestamp":1598706495947,"user_tz":-180,"elapsed":4559,"user":{"displayName":"paul mwaura","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghm4jvoxWah4odG51JJguSfpGKOQt0BQt0ME7NByw=s64","userId":"05571276976991411894"}},"outputId":"8c091de4-d3ec-4d2c-ff75-7ad8b647b198"},"source":["# Preview the top rows.\n","#\n","print(len(documents))\n","print(documents[:5])"],"execution_count":2,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-8cc9cce73421>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Preview the top rows.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'documents' is not defined"]}]},{"cell_type":"markdown","metadata":{"id":"A0vkgCtOUsv9","colab_type":"text"},"source":["## Data Pre-processing"]},{"cell_type":"markdown","metadata":{"id":"p-Vk7cS1Zjaw","colab_type":"text"},"source":[">>\n","**Tokenization**: Split the text into sentences and the sentences into words. Lowercase the words and remove punctuation.\n",">>\n","Words that have fewer than 3 characters are removed.\n",">>\n","All stopwords are removed.\n",">>\n","**Words are lemmatized** — words in third person are changed to first person and verbs in past and future tenses are changed into present.\n",">>\n","**Words are stemmed** — words are reduced to their root form."]},{"cell_type":"markdown","metadata":{"id":"FMrgILf7lcEn","colab_type":"text"},"source":["**Loading gensim and nltk libraries**"]},{"cell_type":"code","metadata":{"id":"BWB0eNwMUXTn","colab_type":"code","colab":{},"executionInfo":{"status":"aborted","timestamp":1598706495918,"user_tz":-180,"elapsed":4518,"user":{"displayName":"paul mwaura","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghm4jvoxWah4odG51JJguSfpGKOQt0BQt0ME7NByw=s64","userId":"05571276976991411894"}}},"source":["import gensim\n","from gensim.utils import simple_preprocess\n","from gensim.parsing.preprocessing import STOPWORDS\n","from nltk.stem import WordNetLemmatizer, SnowballStemmer\n","from nltk.stem.porter import *\n","import numpy as np\n","np.random.seed(2018)\n","import nltk\n","nltk.download('wordnet')\n","\n","from nltk import PorterStemmer\n","stemmer=PorterStemmer()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jfSLejlBlxTi","colab_type":"text"},"source":["**Lematization**"]},{"cell_type":"code","metadata":{"id":"oONG8jcZlmg3","colab_type":"code","colab":{},"executionInfo":{"status":"aborted","timestamp":1598706495920,"user_tz":-180,"elapsed":4516,"user":{"displayName":"paul mwaura","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghm4jvoxWah4odG51JJguSfpGKOQt0BQt0ME7NByw=s64","userId":"05571276976991411894"}}},"source":["def lemmatize_stemming(text):\n","    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n","def preprocess(text):**Lematization**\n","    result = []\n","    for token in gensim.utils.simple_preprocess(text):\n","        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n","            result.append(lemmatize_stemming(token))\n","    return result"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XNjh6f7Jl1cp","colab_type":"code","colab":{},"executionInfo":{"status":"aborted","timestamp":1598706495922,"user_tz":-180,"elapsed":4509,"user":{"displayName":"paul mwaura","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghm4jvoxWah4odG51JJguSfpGKOQt0BQt0ME7NByw=s64","userId":"05571276976991411894"}}},"source":["doc_sample = documents[documents['index'] == 4310].values[0][0]\n","print('original document: ')\n","words = []\n","for word in doc_sample.split(' '):\n","    words.append(word)\n","print(words)\n","print('\\n\\n tokenized and lemmatized document: ')\n","print(preprocess(doc_sample))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pquEp9ASoasp","colab_type":"text"},"source":["> Preprocess the headline text, saving the results as ‘processed_docs’"]},{"cell_type":"code","metadata":{"id":"3_K1jQo-l52U","colab_type":"code","colab":{},"executionInfo":{"status":"aborted","timestamp":1598706495923,"user_tz":-180,"elapsed":4501,"user":{"displayName":"paul mwaura","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghm4jvoxWah4odG51JJguSfpGKOQt0BQt0ME7NByw=s64","userId":"05571276976991411894"}}},"source":["processed_docs = documents['headline_text'].map(preprocess)\n","processed_docs[:10]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"R5bz2On1oj12","colab_type":"text"},"source":["## Bag of Words on the Data set"]},{"cell_type":"markdown","metadata":{"id":"ssE5Vq4vol-W","colab_type":"text"},"source":[">>\n","Create a dictionary from ‘processed_docs’ containing the number of times a word appears in the training set."]},{"cell_type":"code","metadata":{"id":"Hc6YDM7gocGY","colab_type":"code","colab":{},"executionInfo":{"status":"aborted","timestamp":1598706495924,"user_tz":-180,"elapsed":4493,"user":{"displayName":"paul mwaura","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghm4jvoxWah4odG51JJguSfpGKOQt0BQt0ME7NByw=s64","userId":"05571276976991411894"}}},"source":["dictionary = gensim.corpora.Dictionary(processed_docs)\n","count = 0\n","for k, v in dictionary.iteritems():\n","    print(k, v)\n","    count += 1\n","    if count > 10:\n","        break"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qHGMsS39o7wy","colab_type":"text"},"source":["**Gensim filter_extremes**\n",">>\n","Filter out tokens that appear in:\n",">>\n","* less than 15 documents (absolute number) or\n",">>\n","* more than 0.5 documents (fraction of total corpus size, not absolute number).\n",">>\n","* after the above two steps, keep only the first 100000 most frequent tokens."]},{"cell_type":"code","metadata":{"id":"qm-V8H-uov7W","colab_type":"code","colab":{},"executionInfo":{"status":"aborted","timestamp":1598706495925,"user_tz":-180,"elapsed":4490,"user":{"displayName":"paul mwaura","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghm4jvoxWah4odG51JJguSfpGKOQt0BQt0ME7NByw=s64","userId":"05571276976991411894"}}},"source":["dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cc3SenzRpZn0","colab_type":"text"},"source":["## Gensim doc2bow"]},{"cell_type":"markdown","metadata":{"id":"sQyvsEQqpW4T","colab_type":"text"},"source":[">>\n","For each document we create a dictionary reporting how many\n","words and how many times those words appear. Save this to ‘bow_corpus’, then check our selected document earlier."]},{"cell_type":"code","metadata":{"id":"yoKiM9xypQPo","colab_type":"code","colab":{},"executionInfo":{"status":"aborted","timestamp":1598706495926,"user_tz":-180,"elapsed":4483,"user":{"displayName":"paul mwaura","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghm4jvoxWah4odG51JJguSfpGKOQt0BQt0ME7NByw=s64","userId":"05571276976991411894"}}},"source":["bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n","bow_corpus[4310]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fsFvSA58sY0Z","colab_type":"text"},"source":["**Preview Bag Of Words for our sample preprocessed document.**"]},{"cell_type":"code","metadata":{"id":"Tcie1RH4petF","colab_type":"code","colab":{},"executionInfo":{"status":"aborted","timestamp":1598706495927,"user_tz":-180,"elapsed":4475,"user":{"displayName":"paul mwaura","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghm4jvoxWah4odG51JJguSfpGKOQt0BQt0ME7NByw=s64","userId":"05571276976991411894"}}},"source":["bow_doc_4310 = bow_corpus[4310]\n","for i in range(len(bow_doc_4310)):\n","    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_4310[i][0], \n","                                               dictionary[bow_doc_4310[i][0]], \n","bow_doc_4310[i][1]))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0XH46vchsiD0","colab_type":"text"},"source":["## TF-IDF"]},{"cell_type":"markdown","metadata":{"id":"OEjFl3XKsfqU","colab_type":"text"},"source":[">>\n","Create tf-idf model object using models.TfidfModel on ‘bow_corpus’ and save it to ‘tfidf’, then apply transformation to the entire corpus and call it ‘corpus_tfidf’. Finally we preview TF-IDF scores for our first document."]},{"cell_type":"code","metadata":{"id":"YL3RlO4tscnu","colab_type":"code","colab":{},"executionInfo":{"status":"aborted","timestamp":1598706495928,"user_tz":-180,"elapsed":4466,"user":{"displayName":"paul mwaura","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghm4jvoxWah4odG51JJguSfpGKOQt0BQt0ME7NByw=s64","userId":"05571276976991411894"}}},"source":["from gensim import corpora, models\n","tfidf = models.TfidfModel(bow_corpus)\n","corpus_tfidf = tfidf[bow_corpus]\n","from pprint import pprint\n","for doc in corpus_tfidf:\n","    pprint(doc)\n","    break"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"un4ZYyBqssYX","colab_type":"text"},"source":["## Running LDA using Bag of Words"]},{"cell_type":"markdown","metadata":{"id":"EROU9gEEsqb1","colab_type":"text"},"source":[">>\n","Train our lda model using gensim.models.LdaMulticore and save it to ‘lda_model’"]},{"cell_type":"code","metadata":{"id":"JnQNaEXxsmhP","colab_type":"code","colab":{},"executionInfo":{"status":"aborted","timestamp":1598706495929,"user_tz":-180,"elapsed":4463,"user":{"displayName":"paul mwaura","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghm4jvoxWah4odG51JJguSfpGKOQt0BQt0ME7NByw=s64","userId":"05571276976991411894"}}},"source":["lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=10, id2word=dictionary, passes=2, workers=2)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fgABOOnTs0We","colab_type":"text"},"source":[">>\n","For each topic, we will explore the words occuring in that topic and its relative weight."]},{"cell_type":"code","metadata":{"id":"5FJbBIN8sxGs","colab_type":"code","colab":{},"executionInfo":{"status":"aborted","timestamp":1598706495932,"user_tz":-180,"elapsed":4458,"user":{"displayName":"paul mwaura","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghm4jvoxWah4odG51JJguSfpGKOQt0BQt0ME7NByw=s64","userId":"05571276976991411894"}}},"source":["for idx, topic in lda_model.print_topics(-1):\n","    print('Topic: {} \\nWords: {}'.format(idx, topic))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"khJTFUUmtDnO","colab_type":"text"},"source":["> We can now distinguish different topics using the words in each topic and their corresponding weights.\n"]},{"cell_type":"markdown","metadata":{"id":"NSPZkwUgtQZ1","colab_type":"text"},"source":["## Running LDA using TF-IDF"]},{"cell_type":"code","metadata":{"id":"OiErjhVGs4A5","colab_type":"code","colab":{},"executionInfo":{"status":"aborted","timestamp":1598706495934,"user_tz":-180,"elapsed":4451,"user":{"displayName":"paul mwaura","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghm4jvoxWah4odG51JJguSfpGKOQt0BQt0ME7NByw=s64","userId":"05571276976991411894"}}},"source":["lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=10, id2word=dictionary, passes=2, workers=4)\n","for idx, topic in lda_model_tfidf.print_topics(-1):\n","    print('Topic: {} Word: {}'.format(idx, topic))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wwlJz2gEtXCH","colab_type":"text"},"source":["> We can distinguish different topics using the words in each topic and their corresponding weights."]},{"cell_type":"markdown","metadata":{"id":"Cy8e0dmathrS","colab_type":"text"},"source":["## Performance evaluation by classifying sample document using LDA Bag of Words model"]},{"cell_type":"code","metadata":{"id":"jFtZjWPitS_Y","colab_type":"code","colab":{},"executionInfo":{"status":"aborted","timestamp":1598706495935,"user_tz":-180,"elapsed":4443,"user":{"displayName":"paul mwaura","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghm4jvoxWah4odG51JJguSfpGKOQt0BQt0ME7NByw=s64","userId":"05571276976991411894"}}},"source":["processed_docs[4310]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"M8j7yKgytmtd","colab_type":"code","colab":{},"executionInfo":{"status":"aborted","timestamp":1598706495937,"user_tz":-180,"elapsed":4436,"user":{"displayName":"paul mwaura","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghm4jvoxWah4odG51JJguSfpGKOQt0BQt0ME7NByw=s64","userId":"05571276976991411894"}}},"source":["for index, score in sorted(lda_model[bow_corpus[4310]], key=lambda tup: -1*tup[1]):\n","    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model.print_topic(index, 10)))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cvJkuK5Bt6lg","colab_type":"text"},"source":["> Our test document has the highest probability to be part of the topic that our model assigned, which is the accurate classification."]},{"cell_type":"markdown","metadata":{"id":"QHn_KyW-t-I5","colab_type":"text"},"source":["## Performance evaluation by classifying sample document using LDA TF-IDF model."]},{"cell_type":"code","metadata":{"id":"zK9foBYgtwh3","colab_type":"code","colab":{},"executionInfo":{"status":"aborted","timestamp":1598706495938,"user_tz":-180,"elapsed":4427,"user":{"displayName":"paul mwaura","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghm4jvoxWah4odG51JJguSfpGKOQt0BQt0ME7NByw=s64","userId":"05571276976991411894"}}},"source":["for index, score in sorted(lda_model_tfidf[bow_corpus[4310]], key=lambda tup: -1*tup[1]):\n","    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model_tfidf.print_topic(index, 10)))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YCDepPwxuLlo","colab_type":"text"},"source":["> Our test document has the highest probability to be part of the topic that our model assigned, which is the accurate classification."]},{"cell_type":"markdown","metadata":{"id":"3tjIU_dTuVkM","colab_type":"text"},"source":["## Testing model on unseen document"]},{"cell_type":"code","metadata":{"id":"2mFt6wuquEn6","colab_type":"code","colab":{},"executionInfo":{"status":"aborted","timestamp":1598706495943,"user_tz":-180,"elapsed":4423,"user":{"displayName":"paul mwaura","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghm4jvoxWah4odG51JJguSfpGKOQt0BQt0ME7NByw=s64","userId":"05571276976991411894"}}},"source":["# unseen_document = 'How a Pentagon deal became an identity crisis for Google'\n","\n","unseen_document = input(\"Enter a sentence to identify the topic: \")\n","\n","bow_vector = dictionary.doc2bow(preprocess(unseen_document))\n","for index, score in sorted(lda_model[bow_vector], key=lambda tup: -1*tup[1]):\n","    print(\"Score: {}\\t Topic: {}\".format(score, lda_model.print_topic(index, 5)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fkgQtjXtxKBk","colab_type":"code","colab":{},"executionInfo":{"status":"aborted","timestamp":1598706495945,"user_tz":-180,"elapsed":4421,"user":{"displayName":"paul mwaura","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghm4jvoxWah4odG51JJguSfpGKOQt0BQt0ME7NByw=s64","userId":"05571276976991411894"}}},"source":[""],"execution_count":null,"outputs":[]}]}